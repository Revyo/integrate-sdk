---
title: Cloudflare Workers AI
description: Use MCP tools with Cloudflare Workers AI
---

The Integrate SDK provides seamless integration with Cloudflare Workers AI, allowing AI models running on Cloudflare to access your integrations through MCP tools.

## Installation

Install the Integrate SDK and Cloudflare Workers types:

```bash
bun add integrate-sdk @cloudflare/workers-types
```

## Setup

### Basic Worker Example

```typescript
// worker.ts
import { Ai } from "@cloudflare/ai";
import { createMCPClient, githubIntegration } from "integrate-sdk";
import { getCloudflareTools } from "integrate-sdk/server";

export interface Env {
  AI: any;
  GITHUB_CLIENT_ID: string;
}

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const client = createMCPClient({
      integrations: [githubIntegration({ clientId: env.GITHUB_CLIENT_ID })],
    });

    const tools = await getCloudflareTools(client);
    const ai = new Ai(env.AI);

    const response = await ai.run("@cf/meta/llama-3-8b-instruct", {
      messages: [
        {
          role: "system",
          content: "You are a helpful assistant with access to GitHub.",
        },
        {
          role: "user",
          content: "List my GitHub repositories",
        },
      ],
      tools: Object.values(tools),
    });

    return Response.json(response);
  },
};
```

## Server-Side with Token Injection

For Next.js or other frameworks running on Cloudflare:

### 1. Server Configuration

```typescript
// lib/integrate.ts
import { createMCPServer, githubIntegration } from "integrate-sdk/server";

export const { client: serverClient } = createMCPServer({
  apiKey: process.env.INTEGRATE_API_KEY,
  integrations: [
    githubIntegration({
      scopes: ["repo", "user"],
    }),
  ],
});
```

### 2. AI API Route

```typescript
// app/api/ai/route.ts
import { serverClient } from "@/lib/integrate";
import {
  getCloudflareTools,
  executeCloudflareToolCall,
} from "integrate-sdk/server";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const tools = await getCloudflareTools(serverClient);

  const response = await env.AI.run("@cf/meta/llama-3-8b-instruct", {
    messages,
    tools: Object.values(tools),
  });

  if (response.tool_calls) {
    for (const toolCall of response.tool_calls) {
      await executeCloudflareToolCall(serverClient, toolCall);
    }
  }

  return Response.json(response);
}
```

## Supported Models

Cloudflare Workers AI supports function calling with models like:

- `@cf/meta/llama-3-8b-instruct`
- `@cf/meta/llama-3-70b-instruct`
- And other compatible models

Check [Cloudflare's documentation](https://developers.cloudflare.com/workers-ai/) for the latest supported models.

## Next Steps

- Explore [Advanced Configuration](/docs/getting-started/advanced-usage)
- Check the [API Reference](/docs/reference/options) for complete type definitions
